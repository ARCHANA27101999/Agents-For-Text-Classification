{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzAQg358KWPj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMeq2LvddjWJ",
    "outputId": "b044cfb4-e1dd-4087-dbfc-51df634599f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1100\n",
      "Using model: llama-3.1-8b-instant\n",
      "Done inference. API calls made: 110\n",
      "Saved predictions to /content/multitask_predictions_batched.csv\n",
      "\n",
      "\n",
      "====== METRICS ======\n",
      "\n",
      "\n",
      "--- Task: SA (n=300) ---\n",
      "Accuracy: 0.5233\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Mixed_feelings       0.44      0.15      0.22       100\n",
      "      Negative       0.57      0.67      0.61       100\n",
      "      Positive       0.51      0.75      0.60       100\n",
      "\n",
      "      accuracy                           0.52       300\n",
      "     macro avg       0.51      0.52      0.48       300\n",
      "  weighted avg       0.51      0.52      0.48       300\n",
      "\n",
      "\n",
      "--- Task: OFFENS (n=500) ---\n",
      "Accuracy: 0.278\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.34      0.44      0.38       100\n",
      "                           Offensive       0.26      0.18      0.21       100\n",
      "     Offensive_Targeted_Insult_Group       0.29      0.16      0.21       100\n",
      "Offensive_Targeted_Insult_Individual       0.25      0.60      0.35       100\n",
      "               Offensive_Untargetede       0.20      0.01      0.02       100\n",
      "\n",
      "                            accuracy                           0.28       500\n",
      "                           macro avg       0.27      0.28      0.23       500\n",
      "                        weighted avg       0.27      0.28      0.23       500\n",
      "\n",
      "\n",
      "--- Task: OTHERCAT (n=300) ---\n",
      "Accuracy: 0.6367\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Homophobia       0.67      0.58      0.62       100\n",
      "Non-anti-LGBT+ content       0.61      0.69      0.65       100\n",
      "           Transphobia       0.64      0.64      0.64       100\n",
      "\n",
      "              accuracy                           0.64       300\n",
      "             macro avg       0.64      0.64      0.64       300\n",
      "          weighted avg       0.64      0.64      0.64       300\n",
      "\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q langchain-groq pandas scikit-learn tqdm backoff\n",
    "\n",
    "import os, re, json, time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import backoff\n",
    "\n",
    "\n",
    "CSV_PATH = \"/content/final_multitask___test.csv\"   \n",
    "OUT_CSV  = \"/content/multitask_predictions_batched.csv\"\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise RuntimeError(\"Set GROQ_API_KEY in environment, e.g. os.environ['GROQ_API_KEY']='gsk_...'\")\n",
    "\n",
    "PRIMARY_MODEL = \"llama-3.1-8b-instant\"         \n",
    "FALLBACK_MODEL = \"mixtral-8x7b-instant\"         \n",
    "\n",
    "BATCH_SIZE = 10      \n",
    "PAUSE_BETWEEN_CALLS = 0.2  \n",
    "SAMPLE_N = 0           \n",
    "\n",
    "\n",
    "SENT_LABELS = [\"Mixed_feelings\", \"Negative\", \"Positive\"]\n",
    "\n",
    "OFF_LABELS = [\n",
    "    \"Not_offensive\",\n",
    "    \"Offensive\",\n",
    "    \"Offensive_Targeted_Insult_Group\",\n",
    "    \"Offensive_Targeted_Insult_Individual\",\n",
    "    \"Offensive_Untargetede\"   \n",
    "]\n",
    "\n",
    "ID_LABELS = [\"Homophobia\", \"Non-anti-LGBT+ content\", \"Transphobia\"]\n",
    "\n",
    "TASK_LABELS = {\n",
    "    \"SA\": SENT_LABELS,\n",
    "    \"OFFENS\": OFF_LABELS,\n",
    "    \"OTHERCAT\": ID_LABELS\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "\n",
    "for c in (\"text\",\"label\",\"task\"):\n",
    "    if c not in df.columns:\n",
    "        raise RuntimeError(f\"Expected column '{c}' in CSV but not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "if SAMPLE_N and SAMPLE_N>0:\n",
    "    df = df.head(SAMPLE_N)\n",
    "    print(\"Running on sample rows:\", len(df))\n",
    "\n",
    "\n",
    "def build_llm(model_name):\n",
    "    try:\n",
    "        return ChatGroq(model=model_name, groq_api_key=GROQ_API_KEY, temperature=0), model_name\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not init model {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    llm, current_model = build_llm(PRIMARY_MODEL)\n",
    "    print(f\"Using model: {current_model}\")\n",
    "except Exception:\n",
    "    try:\n",
    "        llm, current_model = build_llm(FALLBACK_MODEL)\n",
    "        print(f\"Primary model failed; switched to fallback: {current_model}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Both primary ({PRIMARY_MODEL}) and fallback ({FALLBACK_MODEL}) failed to initialize: {e}\")\n",
    "\n",
    "\n",
    "def try_extract_json_array(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    m = re.search(r\"\\[.*\\]\", s, flags=re.S)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        arr = json.loads(m.group(0))\n",
    "        if isinstance(arr, list):\n",
    "            return arr\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def try_extract_json_label(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    m = re.search(r\"\\{.*?\\}\", s, flags=re.S)\n",
    "    if not m:\n",
    "        return s.strip()\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        for k in (\"label\",\"classification\",\"class\"):\n",
    "            if k in obj:\n",
    "                return str(obj[k]).strip()\n",
    "        vals = list(obj.values())\n",
    "        if vals:\n",
    "            return str(vals[0]).strip()\n",
    "    except Exception:\n",
    "        return s.strip()\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_for_matching(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    s = s.replace(\"+\",\"plus\")\n",
    "    s = re.sub(r\"[^a-z0-9]+\",\" \", s)\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def force_to_allowed(pred_raw: str, allowed_labels: list):\n",
    "    if not pred_raw or not allowed_labels:\n",
    "        return allowed_labels[0] if allowed_labels else \"\"\n",
    "    pred = try_extract_json_label(pred_raw)\n",
    "    for lab in allowed_labels:\n",
    "        if pred == lab:\n",
    "            return lab\n",
    "    for lab in allowed_labels:\n",
    "        if pred.lower() == lab.lower():\n",
    "            return lab\n",
    "    n_pred = normalize_for_matching(pred)\n",
    "    for lab in allowed_labels:\n",
    "        if normalize_for_matching(lab) == n_pred:\n",
    "            return lab\n",
    "    for lab in allowed_labels:\n",
    "        if normalize_for_matching(lab) in n_pred or n_pred in normalize_for_matching(lab):\n",
    "            return lab\n",
    "    pred_tokens = set(n_pred.split())\n",
    "    best = None; best_score = 0\n",
    "    for lab in allowed_labels:\n",
    "        lab_tokens = set(normalize_for_matching(lab).split())\n",
    "        score = len(pred_tokens & lab_tokens)\n",
    "        if score > best_score:\n",
    "            best_score = score; best = lab\n",
    "    if best_score>0 and best:\n",
    "        return best\n",
    "    return allowed_labels[0]\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_tries=8, max_time=600)\n",
    "def invoke_model_with_retry(prompt: str):\n",
    "    global llm, current_model\n",
    "    try:\n",
    "        resp = llm.invoke(prompt)\n",
    "        return resp.content\n",
    "    except Exception as e:\n",
    "        msg = str(e).lower()\n",
    "        \n",
    "        if \"model\" in msg and (\"not found\" in msg or \"does not exist\" in msg or \"you do not have access\" in msg):\n",
    "            if current_model != FALLBACK_MODEL:\n",
    "                print(f\"[INFO] model error: {e} -> switching to fallback model {FALLBACK_MODEL}\")\n",
    "                llm = ChatGroq(model=FALLBACK_MODEL, groq_api_key=GROQ_API_KEY, temperature=0)\n",
    "                current_model = FALLBACK_MODEL\n",
    "       \n",
    "                raise\n",
    " \n",
    "        raise\n",
    "\n",
    "\n",
    "def build_batch_prompt(texts, tasks):\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(\"You are a precise classifier. For each input, choose exactly ONE label corresponding to its task.\")\n",
    "    prompt_lines.append(\"Return EXACTLY one JSON array (no extra text) with the same number of items as inputs.\")\n",
    "    prompt_lines.append(\"Each array element must be a JSON object with a single key 'label', e.g. {\\\"label\\\":\\\"Positive\\\"}.\")\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"Label sets (exact spellings):\")\n",
    "    prompt_lines.append(\"SA (Sentiment): \" + \", \".join([f'\"{l}\"' for l in SENT_LABELS]))\n",
    "    prompt_lines.append(\"OFFENS (Offensive): \" + \", \".join([f'\"{l}\"' for l in OFF_LABELS]))\n",
    "    prompt_lines.append(\"OTHERCAT (Identity): \" + \", \".join([f'\"{l}\"' for l in ID_LABELS]))\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"Now classify the following inputs in order. For each item, I label its task.\")\n",
    "    prompt_lines.append(\"\")\n",
    "    for i,(t,task) in enumerate(zip(texts,tasks), start=1):\n",
    "        safe_text = t.replace('\"\"\"', '\\\"\\\"\\\"')\n",
    "        prompt_lines.append(f\"{i}) Task={task} --- Text:\\n\\\"\\\"\\\"{safe_text}\\\"\\\"\\\"\\n\")\n",
    "    prompt_lines.append(\"\")\n",
    "    prompt_lines.append(\"Return only a single JSON array. Example:\")\n",
    "    prompt_lines.append('[{\"label\":\"Positive\"}, {\"label\":\"Negative\"}, {\"label\":\"Mixed_feelings\"}]')\n",
    "    prompt_lines.append(\"\")\n",
    "    return \"\\n\".join(prompt_lines)\n",
    "\n",
    "\n",
    "rows = df.reset_index(drop=True)\n",
    "n = len(rows)\n",
    "preds = [\"\"] * n\n",
    "raws  = [\"\"] * n\n",
    "\n",
    "i = 0\n",
    "calls = 0\n",
    "while i < n:\n",
    "    batch_texts = []\n",
    "    batch_tasks = []\n",
    "    idxs = []\n",
    "    for j in range(i, min(n, i + BATCH_SIZE)):\n",
    "        batch_texts.append(str(rows.at[j, \"text\"]))\n",
    "        batch_tasks.append(str(rows.at[j, \"task\"]))\n",
    "        idxs.append(j)\n",
    "    prompt = build_batch_prompt(batch_texts, batch_tasks)\n",
    "    try:\n",
    "        raw = invoke_model_with_retry(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Batch call failed at indices {idxs}: {e}\")\n",
    "        # Per-row fallback\n",
    "        for k, idx in enumerate(idxs):\n",
    "            single_prompt = build_batch_prompt([batch_texts[k]], [batch_tasks[k]])\n",
    "            try:\n",
    "                r = invoke_model_with_retry(single_prompt)\n",
    "                arr = try_extract_json_array(r)\n",
    "                if arr and isinstance(arr, list) and len(arr) >= 1:\n",
    "                    label_raw = arr[0].get(\"label\", None) if isinstance(arr[0], dict) else None\n",
    "                    if label_raw is None:\n",
    "                        label_raw = try_extract_json_label(r)\n",
    "                else:\n",
    "                    label_raw = try_extract_json_label(r)\n",
    "            except Exception as e2:\n",
    "                print(f\"[FALLBACK ERROR] single-row failed idx {idx}: {e2}\")\n",
    "                label_raw = \"\"\n",
    "            allowed = TASK_LABELS.get(batch_tasks[k], [])\n",
    "            forced = force_to_allowed(label_raw, allowed) if allowed else label_raw\n",
    "            preds[idx] = forced\n",
    "            raws[idx] = label_raw\n",
    "        i += BATCH_SIZE\n",
    "        continue\n",
    "\n",
    "    calls += 1\n",
    "    arr = try_extract_json_array(raw)\n",
    "    if arr and isinstance(arr, list) and len(arr) == len(idxs):\n",
    "        for k, idx in enumerate(idxs):\n",
    "            el = arr[k]\n",
    "            if isinstance(el, dict) and \"label\" in el:\n",
    "                label_raw = el[\"label\"]\n",
    "            else:\n",
    "                label_raw = el if isinstance(el, str) else json.dumps(el)\n",
    "            allowed = TASK_LABELS.get(batch_tasks[k], [])\n",
    "            forced = force_to_allowed(label_raw, allowed) if allowed else label_raw\n",
    "            preds[idx] = forced\n",
    "            raws[idx]  = label_raw\n",
    "    else:\n",
    "        js_objs = re.findall(r\"\\{.*?\\}\", raw, flags=re.S)\n",
    "        if js_objs and len(js_objs) >= len(idxs):\n",
    "            for k, idx in enumerate(idxs):\n",
    "                try:\n",
    "                    obj = json.loads(js_objs[k])\n",
    "                    label_raw = obj.get(\"label\", None) or obj.get(\"classification\", None) or next(iter(obj.values()), \"\")\n",
    "                except Exception:\n",
    "                    label_raw = try_extract_json_label(js_objs[k])\n",
    "                allowed = TASK_LABELS.get(batch_tasks[k], [])\n",
    "                forced = force_to_allowed(label_raw, allowed) if allowed else label_raw\n",
    "                preds[idx] = forced\n",
    "                raws[idx]  = label_raw\n",
    "        else:\n",
    "            lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
    "            candidate_lines = []\n",
    "            for ln in lines[::-1]:\n",
    "                if ln.startswith(\"[\") or ln.startswith(\"]\") or \"return\" in ln.lower():\n",
    "                    continue\n",
    "                candidate_lines.append(ln)\n",
    "                if len(candidate_lines) >= len(idxs):\n",
    "                    break\n",
    "            candidate_lines = candidate_lines[::-1]\n",
    "            for k, idx in enumerate(idxs):\n",
    "                label_raw = candidate_lines[k] if k < len(candidate_lines) else \"\"\n",
    "                allowed = TASK_LABELS.get(batch_tasks[k], [])\n",
    "                forced = force_to_allowed(label_raw, allowed) if allowed else label_raw\n",
    "                preds[idx] = forced\n",
    "                raws[idx]  = label_raw\n",
    "\n",
    "    time.sleep(PAUSE_BETWEEN_CALLS)\n",
    "    i += BATCH_SIZE\n",
    "\n",
    "print(f\"Done inference. API calls made: {calls}\")\n",
    "\n",
    "\n",
    "df_out = rows.copy()\n",
    "df_out[\"predicted\"] = preds\n",
    "df_out[\"raw_model_output\"] = raws\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved predictions to\", OUT_CSV)\n",
    "\n",
    "\n",
    "print(\"\\n\\n====== METRICS ======\\n\")\n",
    "for task_name in [\"SA\",\"OFFENS\",\"OTHERCAT\"]:\n",
    "    sub = df_out[df_out[\"task\"] == task_name]\n",
    "    if len(sub) == 0:\n",
    "        print(f\"Task {task_name}: NO SAMPLES FOUND (skipping).\")\n",
    "        continue\n",
    "    y_true = sub[\"label\"].astype(str).tolist()\n",
    "    y_pred = sub[\"predicted\"].astype(str).tolist()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\n--- Task: {task_name} (n={len(sub)}) ---\")\n",
    "    print(\"Accuracy:\", round(acc,4))\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsht5I4Lfea1",
    "outputId": "c7f7dbf6-7abe-4fdd-a97b-c8036fcbfcff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1100\n",
      "Graph built successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [38:45<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: /content/predictions_langgraph.csv\n",
      "\n",
      "===== METRICS =====\n",
      "\n",
      "\n",
      "--- Task SA (n=300) ---\n",
      "Accuracy: 0.43333333333333335\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Mixed_feelings       0.36      0.84      0.51       100\n",
      "      Negative       0.60      0.21      0.31       100\n",
      "      Positive       0.74      0.25      0.37       100\n",
      "\n",
      "      accuracy                           0.43       300\n",
      "     macro avg       0.57      0.43      0.40       300\n",
      "  weighted avg       0.57      0.43      0.40       300\n",
      "\n",
      "\n",
      "--- Task OFFENS (n=500) ---\n",
      "Accuracy: 0.316\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.42      0.36      0.39       100\n",
      "                           Offensive       0.00      0.00      0.00       100\n",
      "     Offensive_Targeted_Insult_Group       0.25      0.62      0.35       100\n",
      "Offensive_Targeted_Insult_Individual       0.33      0.29      0.31       100\n",
      "               Offensive_Untargetede       0.41      0.31      0.35       100\n",
      "\n",
      "                            accuracy                           0.32       500\n",
      "                           macro avg       0.28      0.32      0.28       500\n",
      "                        weighted avg       0.28      0.32      0.28       500\n",
      "\n",
      "\n",
      "--- Task OTHERCAT (n=300) ---\n",
      "Accuracy: 0.49333333333333335\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Homophobia       1.00      0.02      0.04       100\n",
      "Non-anti-LGBT+ content       0.44      0.99      0.61       100\n",
      "           Transphobia       0.65      0.47      0.55       100\n",
      "\n",
      "              accuracy                           0.49       300\n",
      "             macro avg       0.70      0.49      0.40       300\n",
      "          weighted avg       0.70      0.49      0.40       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q langgraph langchain-groq pandas scikit-learn tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "CSV_PATH = \"/content/final_multitask___test.csv\"\n",
    "OUT_CSV  = \"/content/predictions_langgraph.csv\"\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    raise RuntimeError(\"Set environment variable: os.environ['GROQ_API_KEY']='your-key'\")\n",
    "\n",
    "MODEL = \"llama-3.1-8b-instant\"\n",
    "\n",
    "\n",
    "SENT_LABELS = [\"Mixed_feelings\", \"Negative\", \"Positive\"]\n",
    "\n",
    "OFF_LABELS = [\n",
    "    \"Not_offensive\",\n",
    "    \"Offensive\",\n",
    "    \"Offensive_Targeted_Insult_Group\",\n",
    "    \"Offensive_Targeted_Insult_Individual\",\n",
    "    \"Offensive_Untargetede\"\n",
    "]\n",
    "\n",
    "ID_LABELS = [\"Homophobia\", \"Non-anti-LGBT+ content\", \"Transphobia\"]\n",
    "\n",
    "\n",
    "TASK_TO_LABELS = {\n",
    "    \"SA\": SENT_LABELS,\n",
    "    \"OFFENS\": OFF_LABELS,\n",
    "    \"OTHERCAT\": ID_LABELS\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "\n",
    "for c in (\"text\", \"label\", \"task\"):\n",
    "    if c not in df.columns:\n",
    "        raise RuntimeError(f\"Missing required column: {c}\")\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=MODEL,\n",
    "    groq_api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "\n",
    "def extract_label(text):\n",
    "    \"\"\"Extract label from JSON or fallback to raw.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    m = re.search(r\"\\{.*?\\}\", text, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            if \"label\" in obj:\n",
    "                return obj[\"label\"]\n",
    "        except:\n",
    "            pass\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def build_prompt(task, text):\n",
    "    allowed = TASK_TO_LABELS[task]\n",
    "    labels_str = \", \".join([f'\"{l}\"' for l in allowed])\n",
    "    return f\"\"\"\n",
    "Classify the following text according to task = {task}.\n",
    "Allowed labels = [{labels_str}]\n",
    "\n",
    "Return ONLY JSON:\n",
    "{{ \"label\": \"<label>\" }}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task: str\n",
    "    text: str\n",
    "    prediction: str\n",
    "\n",
    "\n",
    "def router_node(state: State):\n",
    "    return {\"messages\": [f\"ROUTE:{state['task']}\"]}\n",
    "\n",
    "\n",
    "def sentiment_agent(state: State):\n",
    "    prompt = build_prompt(\"SA\", state[\"text\"])\n",
    "    out = llm.invoke(prompt).content\n",
    "    label = extract_label(out)\n",
    "    return {\"prediction\": label, \"messages\": [out]}\n",
    "\n",
    "\n",
    "def offense_agent(state: State):\n",
    "    prompt = build_prompt(\"OFFENS\", state[\"text\"])\n",
    "    out = llm.invoke(prompt).content\n",
    "    label = extract_label(out)\n",
    "    return {\"prediction\": label, \"messages\": [out]}\n",
    "\n",
    "\n",
    "def othercat_agent(state: State):\n",
    "    prompt = build_prompt(\"OTHERCAT\", state[\"text\"])\n",
    "    out = llm.invoke(prompt).content\n",
    "    label = extract_label(out)\n",
    "    return {\"prediction\": label, \"messages\": [out]}\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"router\", router_node)\n",
    "graph.add_node(\"sentiment\", sentiment_agent)\n",
    "graph.add_node(\"offense\", offense_agent)\n",
    "graph.add_node(\"othercat\", othercat_agent)\n",
    "\n",
    "graph.add_edge(START, \"router\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda st: st[\"task\"],\n",
    "    {\n",
    "        \"SA\": \"sentiment\",\n",
    "        \"OFFENS\": \"offense\",\n",
    "        \"OTHERCAT\": \"othercat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"sentiment\", END)\n",
    "graph.add_edge(\"offense\", END)\n",
    "graph.add_edge(\"othercat\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "print(\"Graph built successfully.\")\n",
    "\n",
    "\n",
    "preds = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    out = app.invoke({\n",
    "        \"messages\": [],\n",
    "        \"task\": row[\"task\"],\n",
    "        \"text\": row[\"text\"]\n",
    "    })\n",
    "    preds.append(out[\"prediction\"])\n",
    "\n",
    "df[\"predicted\"] = preds\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved predictions to:\", OUT_CSV)\n",
    "\n",
    "\n",
    "print(\"\\n===== METRICS =====\\n\")\n",
    "for task_name in [\"SA\", \"OFFENS\", \"OTHERCAT\"]:\n",
    "    sub = df[df[\"task\"] == task_name]\n",
    "    if len(sub)==0:\n",
    "        continue\n",
    "    print(f\"\\n--- Task {task_name} (n={len(sub)}) ---\")\n",
    "    print(\"Accuracy:\", accuracy_score(sub[\"label\"], sub[\"predicted\"]))\n",
    "    print(classification_report(sub[\"label\"], sub[\"predicted\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7yZVadSMpuY",
    "outputId": "e5773ee6-6aac-461d-af4a-8de0cbbb7bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1100\n",
      "Using model: gemini-2.5-flash\n",
      "[WARN] JSON parse fallback\n",
      "Inference complete. Calls: 110\n",
      "Saved to: /content/multitask_predictions_gemini.csv\n",
      "\n",
      "===== METRICS =====\n",
      "\n",
      "--- SA (n=300) ---\n",
      "Accuracy: 0.5933\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Mixed_feelings       0.55      0.21      0.30       100\n",
      "      Negative       0.58      0.70      0.64       100\n",
      "      Positive       0.61      0.87      0.72       100\n",
      "\n",
      "      accuracy                           0.59       300\n",
      "     macro avg       0.58      0.59      0.55       300\n",
      "  weighted avg       0.58      0.59      0.55       300\n",
      "\n",
      "\n",
      "--- OFFENS (n=500) ---\n",
      "Accuracy: 0.33\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.34      0.94      0.50       100\n",
      "                           Offensive       0.24      0.04      0.07       100\n",
      "     Offensive_Targeted_Insult_Group       0.44      0.31      0.36       100\n",
      "Offensive_Targeted_Insult_Individual       0.25      0.29      0.27       100\n",
      "               Offensive_Untargetede       0.35      0.07      0.12       100\n",
      "\n",
      "                            accuracy                           0.33       500\n",
      "                           macro avg       0.32      0.33      0.26       500\n",
      "                        weighted avg       0.32      0.33      0.26       500\n",
      "\n",
      "\n",
      "--- OTHERCAT (n=300) ---\n",
      "Accuracy: 0.7933\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Homophobia       0.77      0.83      0.80       100\n",
      "Non-anti-LGBT+ content       0.69      0.73      0.71       100\n",
      "           Transphobia       0.95      0.82      0.88       100\n",
      "\n",
      "              accuracy                           0.79       300\n",
      "             macro avg       0.80      0.79      0.80       300\n",
      "          weighted avg       0.80      0.79      0.80       300\n",
      "\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, re, json, time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import backoff\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "CSV_PATH = \"/content/final_multitask___test.csv\"     \n",
    "OUT_CSV  = \"/content/multitask_predictions_gemini.csv\"\n",
    "\n",
    "PRIMARY_MODEL  = \"gemini-2.5-flash\"\n",
    "FALLBACK_MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "PAUSE_BETWEEN_CALLS = 0.2\n",
    "SAMPLE_N = 0  \n",
    "\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "SENT_LABELS = [\"Mixed_feelings\", \"Negative\", \"Positive\"]\n",
    "\n",
    "OFF_LABELS = [\n",
    "    \"Not_offensive\",\n",
    "    \"Offensive\",\n",
    "    \"Offensive_Targeted_Insult_Group\",\n",
    "    \"Offensive_Targeted_Insult_Individual\",\n",
    "    \"Offensive_Untargetede\"\n",
    "]\n",
    "\n",
    "ID_LABELS = [\"Homophobia\", \"Non-anti-LGBT+ content\", \"Transphobia\"]\n",
    "\n",
    "TASK_LABELS = {\n",
    "    \"SA\": SENT_LABELS,\n",
    "    \"OFFENS\": OFF_LABELS,\n",
    "    \"OTHERCAT\": ID_LABELS\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "\n",
    "for c in (\"text\",\"label\",\"task\"):\n",
    "    if c not in df.columns:\n",
    "        raise RuntimeError(f\"Missing column '{c}'\")\n",
    "\n",
    "if SAMPLE_N > 0:\n",
    "    df = df.head(SAMPLE_N)\n",
    "\n",
    "\n",
    "def build_llm(model_name):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        temperature=0\n",
    "    ), model_name\n",
    "\n",
    "try:\n",
    "    llm, current_model = build_llm(PRIMARY_MODEL)\n",
    "    print(f\"Using model: {current_model}\")\n",
    "except:\n",
    "    llm, current_model = build_llm(FALLallback_MODEL)\n",
    "    print(f\"Primary failed. Using fallback: {current_model}\")\n",
    "\n",
    "\n",
    "def try_extract_json_array(s):\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    m = re.search(r\"\\[.*\\]\", s, flags=re.S)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def try_extract_json_label(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    m = re.search(r\"\\{.*?\\}\", s, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0))\n",
    "            for k in (\"label\",\"classification\",\"class\"):\n",
    "                if k in obj:\n",
    "                    return obj[k]\n",
    "        except:\n",
    "            pass\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_for_matching(s):\n",
    "    s = (s or \"\").lower()\n",
    "    s = s.replace(\"+\",\"plus\")\n",
    "    s = re.sub(r\"[^a-z0-9]+\",\" \", s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def force_to_allowed(pred_raw, allowed):\n",
    "    if not allowed:\n",
    "        return pred_raw\n",
    "    pred = try_extract_json_label(pred_raw)\n",
    "    if pred in allowed:\n",
    "        return pred\n",
    "    for lab in allowed:\n",
    "        if pred.lower() == lab.lower():\n",
    "            return lab\n",
    "    n_pred = normalize_for_matching(pred)\n",
    "    for lab in allowed:\n",
    "        if normalize_for_matching(lab) == n_pred:\n",
    "            return lab\n",
    "    tokens_pred = set(n_pred.split())\n",
    "    best=None;score=0\n",
    "    for lab in allowed:\n",
    "        s = len(tokens_pred & set(normalize_for_matching(lab).split()))\n",
    "        if s > score:\n",
    "            best=lab;score=s\n",
    "    return best if best else allowed[0]\n",
    "\n",
    "# ------------------------\n",
    "# RETRY WRAPPER\n",
    "# ------------------------\n",
    "@backoff.on_exception(backoff.expo, Exception, max_tries=8)\n",
    "def invoke_model(prompt):\n",
    "    global llm, current_model\n",
    "    try:\n",
    "        resp = llm.invoke(prompt)\n",
    "        return resp.content\n",
    "    except Exception as e:\n",
    "        msg=str(e).lower()\n",
    "        if \"not found\" in msg or \"access\" in msg:\n",
    "            if current_model != FALLBACK_MODEL:\n",
    "                print(\"Switching to fallback:\", FALLBACK_MODEL)\n",
    "                llm, current_model = build_llm(FALLBACK_MODEL)\n",
    "                raise\n",
    "        raise\n",
    "\n",
    "# ------------------------\n",
    "# BUILD BATCH PROMPT\n",
    "# ------------------------\n",
    "def build_batch_prompt(texts, tasks):\n",
    "    out=[]\n",
    "    out.append(\"You are a strict classifier. Return ONLY one JSON array.\")\n",
    "    out.append('Each element must look like {\"label\":\"Positive\"}.')\n",
    "    out.append(\"\")\n",
    "    out.append(\"Label sets:\")\n",
    "    out.append(\"SA: \" + \", \".join(SENT_LABELS))\n",
    "    out.append(\"OFFENS: \" + \", \".join(OFF_LABELS))\n",
    "    out.append(\"OTHERCAT: \" + \", \".join(ID_LABELS))\n",
    "    out.append(\"\")\n",
    "    out.append(\"Classify in order:\")\n",
    "    out.append(\"\")\n",
    "    for i,(t,tk) in enumerate(zip(texts,tasks), start=1):\n",
    "        safe=t.replace('\"\"\"','\\\"\\\"\\\"')\n",
    "        out.append(f\"{i}) Task={tk}\\n\\\"\\\"\\\"{safe}\\\"\\\"\\\"\")\n",
    "        out.append(\"\")\n",
    "    out.append('Return only this JSON format: [{\"label\":\"Positive\"}]')\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ------------------------\n",
    "# BATCH INFERENCE\n",
    "# ------------------------\n",
    "rows=df.reset_index(drop=True)\n",
    "n=len(rows)\n",
    "\n",
    "preds=[\"\"]*n\n",
    "raws=[\"\"]*n\n",
    "\n",
    "i=0\n",
    "calls=0\n",
    "\n",
    "while i<n:\n",
    "    batch_texts=[]\n",
    "    batch_tasks=[]\n",
    "    idxs=[]\n",
    "\n",
    "    for j in range(i, min(n, i+BATCH_SIZE)):\n",
    "        batch_texts.append(str(rows.at[j,\"text\"]))\n",
    "        batch_tasks.append(str(rows.at[j,\"task\"]))\n",
    "        idxs.append(j)\n",
    "\n",
    "    prompt=build_batch_prompt(batch_texts, batch_tasks)\n",
    "\n",
    "    try:\n",
    "        raw=invoke_model(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed batch {idxs}: {e}\")\n",
    "        i+=BATCH_SIZE\n",
    "        continue\n",
    "\n",
    "    calls+=1\n",
    "    arr=try_extract_json_array(raw)\n",
    "\n",
    "    if arr and len(arr)==len(idxs):\n",
    "        for k,idx in enumerate(idxs):\n",
    "            raw_lab=arr[k].get(\"label\",\"\")\n",
    "            forced=force_to_allowed(raw_lab, TASK_LABELS.get(batch_tasks[k], []))\n",
    "            preds[idx]=forced\n",
    "            raws[idx]=raw_lab\n",
    "    else:\n",
    "        print(\"[WARN] JSON parse fallback\")\n",
    "        for k,idx in enumerate(idxs):\n",
    "            forced=force_to_allowed(raw, TASK_LABELS.get(batch_tasks[k], []))\n",
    "            preds[idx]=forced\n",
    "            raws[idx]=raw\n",
    "\n",
    "    time.sleep(PAUSE_BETWEEN_CALLS)\n",
    "    i+=BATCH_SIZE\n",
    "\n",
    "print(\"Inference complete. Calls:\", calls)\n",
    "\n",
    "\n",
    "df_out=rows.copy()\n",
    "df_out[\"predicted\"]=preds\n",
    "df_out[\"raw_output\"]=raws\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved to:\", OUT_CSV)\n",
    "\n",
    "\n",
    "print(\"\\n===== METRICS =====\")\n",
    "for task in [\"SA\",\"OFFENS\",\"OTHERCAT\"]:\n",
    "    sub=df_out[df_out[\"task\"]==task]\n",
    "    if len(sub)==0:\n",
    "        continue\n",
    "    y_true=sub[\"label\"].astype(str).tolist()\n",
    "    y_pred=sub[\"predicted\"].astype(str).tolist()\n",
    "    print(f\"\\n--- {task} (n={len(sub)}) ---\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true,y_pred),4))\n",
    "    print(classification_report(y_true,y_pred,zero_division=0))\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vL0y5SVIyIqo",
    "outputId": "66e79f63-9628-415b-a4c8-b33dc26fddbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 28600\n",
      "Test rows : 1100\n",
      "Few-shot examples extracted.\n",
      "Using model: gemini-2.5-flash\n",
      "Done. Total calls: 110\n",
      "Saved predictions: /content/multitask_predictions_fewshot.csv\n",
      "\n",
      "===== METRICS =====\n",
      "\n",
      "--- SA ---\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Mixed_feelings       0.52      0.34      0.41       100\n",
      "      Negative       0.64      0.72      0.68       100\n",
      "      Positive       0.66      0.81      0.73       100\n",
      "\n",
      "      accuracy                           0.62       300\n",
      "     macro avg       0.61      0.62      0.61       300\n",
      "  weighted avg       0.61      0.62      0.61       300\n",
      "\n",
      "\n",
      "--- OFFENS ---\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                       Not_offensive       0.40      0.87      0.55       100\n",
      "                           Offensive       0.61      0.19      0.29       100\n",
      "     Offensive_Targeted_Insult_Group       0.45      0.39      0.42       100\n",
      "Offensive_Targeted_Insult_Individual       0.42      0.59      0.49       100\n",
      "               Offensive_Untargetede       0.42      0.11      0.17       100\n",
      "\n",
      "                            accuracy                           0.43       500\n",
      "                           macro avg       0.46      0.43      0.38       500\n",
      "                        weighted avg       0.46      0.43      0.38       500\n",
      "\n",
      "\n",
      "--- OTHERCAT ---\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            Homophobia       0.97      0.86      0.91       100\n",
      "Non-anti-LGBT+ content       0.76      0.94      0.84       100\n",
      "           Transphobia       0.95      0.84      0.89       100\n",
      "\n",
      "              accuracy                           0.88       300\n",
      "             macro avg       0.90      0.88      0.88       300\n",
      "          weighted avg       0.90      0.88      0.88       300\n",
      "\n",
      "\n",
      "All Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, re, json, time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import backoff\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "TRAIN_CSV = \"/content/final_multitask___train.csv\"\n",
    "TEST_CSV  = \"/content/final_multitask___test.csv\"\n",
    "OUT_CSV   = \"/content/multitask_predictions_fewshot.csv\"\n",
    "\n",
    "\n",
    "\n",
    "PRIMARY_MODEL  = \"gemini-2.5-flash\"\n",
    "FALLBACK_MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "PAUSE = 0.2\n",
    "\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(\"Train rows:\", len(train_df))\n",
    "print(\"Test rows :\", len(test_df))\n",
    "\n",
    "\n",
    "SENT_LABELS = [\"Mixed_feelings\", \"Negative\", \"Positive\"]\n",
    "\n",
    "OFF_LABELS = [\n",
    "    \"Not_offensive\",\n",
    "    \"Offensive\",\n",
    "    \"Offensive_Targeted_Insult_Group\",\n",
    "    \"Offensive_Targeted_Insult_Individual\",\n",
    "    \"Offensive_Untargetede\"\n",
    "]\n",
    "\n",
    "ID_LABELS = [\"Homophobia\", \"Non-anti-LGBT+ content\", \"Transphobia\"]\n",
    "\n",
    "TASK_LABELS = {\n",
    "    \"SA\": SENT_LABELS,\n",
    "    \"OFFENS\": OFF_LABELS,\n",
    "    \"OTHERCAT\": ID_LABELS\n",
    "}\n",
    "\n",
    "\n",
    "fewshot = {task:{} for task in TASK_LABELS}\n",
    "\n",
    "for task, labels in TASK_LABELS.items():\n",
    "    for label in labels:\n",
    "        examples = train_df[(train_df[\"task\"]==task) & (train_df[\"label\"]==label)].head(3)\n",
    "        fewshot[task][label] = examples[\"text\"].tolist()\n",
    "\n",
    "print(\"Few-shot examples extracted.\")\n",
    "\n",
    "\n",
    "def build_llm(model):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model,\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        temperature=0\n",
    "    ), model\n",
    "\n",
    "try:\n",
    "    llm, current_model = build_llm(PRIMARY_MODEL)\n",
    "    print(\"Using model:\", current_model)\n",
    "except:\n",
    "    llm, current_model = build_llm(FALLBACK_MODEL)\n",
    "    print(\"Using fallback:\", current_model)\n",
    "\n",
    "\n",
    "def build_prompt(texts, tasks):\n",
    "    out = []\n",
    "    out.append(\"You are a STRICT multilabel classifier.\")\n",
    "    out.append(\"Return ONLY one JSON array.\")\n",
    "    out.append('Each element must be {\"label\":\"LABEL\"}.')\n",
    "\n",
    "    out.append(\"\\n### LABEL SETS\")\n",
    "    out.append(\"SA: \" + \", \".join(SENT_LABELS))\n",
    "    out.append(\"OFFENS: \" + \", \".join(OFF_LABELS))\n",
    "    out.append(\"OTHERCAT: \" + \", \".join(ID_LABELS))\n",
    "\n",
    "    out.append(\"\\n### FEW-SHOT EXAMPLES\")\n",
    "\n",
    "\n",
    "    for task in tasks:\n",
    "        out.append(f\"\\nTask={task} examples:\")\n",
    "        for lab, exs in fewshot[task].items():\n",
    "            for e in exs:\n",
    "                out.append(f\"Example Text: \\\"{e}\\\"\\nLabel: {lab}\\n\")\n",
    "\n",
    "    out.append(\"\\n### NOW CLASSIFY THESE:\")\n",
    "    for i, (txt, tk) in enumerate(zip(texts, tasks), start=1):\n",
    "        safe = txt.replace('\"\"\"', \"'\")\n",
    "        out.append(f\"{i}) Task={tk}\\n\\\"\\\"\\\"{safe}\\\"\\\"\\\"\")\n",
    "\n",
    "    out.append('\\nReturn ONLY JSON like: [{\"label\":\"Positive\"}]')\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "def extract_json_array(s):\n",
    "    m = re.search(r\"\\[.*\\]\", s, flags=re.S)\n",
    "    if not m: return None\n",
    "    try: return json.loads(m.group(0))\n",
    "    except: return None\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_tries=6)\n",
    "def ask_llm(prompt):\n",
    "    global llm, current_model\n",
    "    try:\n",
    "        r = llm.invoke(prompt)\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"not found\" in msg or \"access\" in msg:\n",
    "            if current_model != FALLBACK_MODEL:\n",
    "                llm, current_model = build_llm(FALLBACK_MODEL)\n",
    "                raise\n",
    "        raise\n",
    "\n",
    "\n",
    "rows = test_df.reset_index(drop=True)\n",
    "n = len(rows)\n",
    "\n",
    "preds = [\"\"] * n\n",
    "\n",
    "i = 0\n",
    "calls = 0\n",
    "\n",
    "while i < n:\n",
    "    batch_texts = []\n",
    "    batch_tasks = []\n",
    "    idxs = []\n",
    "\n",
    "    for j in range(i, min(n, i + BATCH_SIZE)):\n",
    "        batch_texts.append(str(rows.at[j,\"text\"]))\n",
    "        batch_tasks.append(str(rows.at[j,\"task\"]))\n",
    "        idxs.append(j)\n",
    "\n",
    "    prompt = build_prompt(batch_texts, batch_tasks)\n",
    "    raw = ask_llm(prompt)\n",
    "\n",
    "    arr = extract_json_array(raw)\n",
    "    if not arr or len(arr) != len(idxs):\n",
    "        print(\"⚠ Fallback parse\")\n",
    "        for k, idx in enumerate(idxs):\n",
    "            preds[idx] = \"Unknown\"\n",
    "    else:\n",
    "        for k, idx in enumerate(idxs):\n",
    "            preds[idx] = arr[k].get(\"label\",\"Unknown\")\n",
    "\n",
    "    i += BATCH_SIZE\n",
    "    time.sleep(PAUSE)\n",
    "    calls += 1\n",
    "\n",
    "print(\"Done. Total calls:\", calls)\n",
    "\n",
    "\n",
    "test_df[\"predicted\"] = preds\n",
    "test_df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved predictions:\", OUT_CSV)\n",
    "\n",
    "print(\"\\n===== METRICS =====\")\n",
    "for task in [\"SA\",\"OFFENS\",\"OTHERCAT\"]:\n",
    "    sub = test_df[test_df[\"task\"] == task]\n",
    "    print(f\"\\n--- {task} ---\")\n",
    "    print(classification_report(sub[\"label\"], sub[\"predicted\"], zero_division=0))\n",
    "\n",
    "print(\"\\nAll Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMcIFxl8H-qP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
